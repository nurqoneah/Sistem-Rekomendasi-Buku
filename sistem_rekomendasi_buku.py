# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m5rPhoWhpEEKA4bKkZEE-GbJU71zr9Q2

# **Proyek Kedua : System Recommender**


### **Nama : Nurul Nyi Qoniah**
### **Email : nurulqoniah313@gmail.com**
### **Username : nurqoneah**

## **Deskripsi**
Membuat model system recommender dengan menggunakan dataset book, user, rating dari [dataset kaggle](https://www.kaggle.com/code/fahadmehfoooz/book-recommendation-system/notebook)

## **Project Overview**

Di era kemajuan informasi, mencari buku yang tepat untuk dibaca bisa menjadi hal yang sangat berharga. Sistem Rekomendasi Buku digunakan untuk membantu pengguna atau pembaca dengan memberikan rekomendasi personal berdasarkan preferensi pengguna, riwayat membaca, dan faktor lain yang relevan. Proyek ini bertujuan untuk mengembangkan sistem rekomendasi buku yang tangguh dengan memanfaatkan teknik-teknik machine learning.

referensi dari proyek ini : [Journal](https://colab.research.google.com/drive/1m5rPhoWhpEEKA4bKkZEE-GbJU71zr9Q2#scrollTo=8Itu1xEASkEl&line=5&uniqifier=1)

## **Business Understanding**

### **Problem Statements**
Bagaimana cara merekomendasikan buku yang disukai pembaca lain dapat direkomendasikan kepada pembaca lainnya juga?

### **Goals**
Dapat membuat sistem rekomendasi yang akurat berdasarkan ratings dan aktivitas pengguna pada masa lalu.

### **Solution approach**
Solusi yang saya buat yaitu dengan menggunakan 2 algoritma Machine Learning sistem rekomendasi,yaitu :

* Content Based Filtering adalah algoritma yang merekomendasikan item merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu.
* Collaborative Filtering adalah algoritma yang bergantung pada pendapat komunitas pengguna.

## **Data Understanding**
Dataset yang digunakan untuk membuat sistem rekomendasi buku ini didapat dari situs Kaggle **Book Recommendation Dataset** yang dapat diakses melalui link [dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset)
"""

import pandas as pd

books = pd.read_csv('/content/Books.csv')
users = pd.read_csv('/content/Users.csv')
ratings = pd.read_csv('/content/Ratings.csv')

print('Jumlah data books : ', len(books.ISBN.unique()))
print('Jumlah data users : ', len(users["User-ID"].unique()))
print('Jumlah data ratings : ', len(ratings.ISBN.unique()))



"""### **Univariate Exploratory Data Analysis**

Feature-feature dalam data **books** sebagai berikut:

*   ISBN : merupakan kode ISBN buku
*   Book_Title : merupakan judul buku
*   Book_Author : merupakan penulis buku
*   Year_Of_Publication : merupakan tahun buku saat dipublikasi
*   Image_URL_S : link gambar buku ukuran kecil
*   Image_URL_M : link gambar buku ukuran medium
*   Image_URL_L : link gambar buku ukuran besar

Feature-feature dalam data **user** sebagai berikut:

*   User-ID : merupkan Id User atau pembaca buku
*   Location : merupakan alamat pembaca buku atau user
*   Age : merupakan umur dari user atau pembaca buku

Feature-feature dalam data **ratings** sebagai berikut:

*   User-ID : merupkan Id User atau pembaca buku
*   ISBN : merupakan kode ISBN buku
*   Book-Rating : merupakan rating user untuk buku
"""

books=books.rename(columns={'Book-Title': 'Book_Title', 'Book-Author': 'Book_Author', "Year-Of-Publication":"Year_Of_Publication", "Image-URL-S":"Image_URL_S", "Image-URL-M":"Image_URL_M","Image-URL-L":"Image_URL_L"})

users=users.rename(columns={'User-ID': 'User_ID'})

ratings=ratings.rename(columns={'User-ID': 'User_ID',"Book-Rating":"Book_Rating"})

books.info()
books.head()
books.describe()
print("\nData null:\n", books.isna().sum())
print("\nJumlah duplikasi: ", books.duplicated().sum())

users.info()
users.head()
users.describe()
print("\nData null:\n", users.isna().sum())
print("\nJumlah duplikasi: ", users.duplicated().sum())

ratings.info()
ratings.head()
ratings.describe()
print("\nData null:\n", ratings.isna().sum())
print("\nJumlah duplikasi: ", ratings.duplicated().sum())

"""
## **Data Preprocessing**
Menggabungkan **rating** dan **user**"""

rating_user = pd.merge(ratings,users, on="User_ID", how="left")
rating_user.head(10)

rating_user.info()

"""Menggabungkan **rating_user** dengan **books**"""

all_data=pd.merge(rating_user, books, on="ISBN", how="right" )

all_data.info()

all_data.head()

all_data.isnull().sum()

"""Menghapus data yang tidak dibutuhkan"""

all_data=all_data.drop(columns=["Image_URL_S", "Image_URL_M", "Image_URL_L","Age","Location"])

all_data = all_data.dropna()

all_data.info()

"""## **Data Preparation**
### **Mengatasi Misiing Value**
menggunkan isnull untuk melihat data yang null
"""

all_data.isnull().sum()

all_data.info()

all_data.head()

all_data.Book_Author.unique()

all_data.head()

"""### **Mensorting data menurut ISBN**"""

preparation = all_data
preparation.sort_values('ISBN')

"""### **Menghapus data duplikat**"""

preparation = preparation.drop_duplicates('ISBN')
preparation = preparation.drop_duplicates('Book_Title')
preparation

"""### **Membuat data list**"""

isbn = preparation['ISBN'].tolist()


book_title = preparation['Book_Title'].tolist()


author = preparation['Book_Author'].tolist()

print(len(isbn))
print(len(book_title))
print(len(author))

"""
## **Modeling and Result**
### **Content Filtered Recommendation System**
teknik content based filtering akan merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu dengan tfidf vectorizer dan menghitung tingkat kesamaan dengan cosine similarity"""

recomendation_data_content=pd.DataFrame({
    'isbn': isbn,
    'book_title': book_title,
    'author': author
})
recomendation_data_content = recomendation_data_content[:20000]

"""### **TfidfVectorizer: mengubah data teks menjadi vektor**"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer()

tf.fit(recomendation_data_content['author'])

tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(recomendation_data_content['author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

tfidf_matrix.todense()

import pandas as pd

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=recomendation_data_content.book_title
).sample(22, axis=1).sample(10, axis=0)

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=recomendation_data_content['book_title'], columns=recomendation_data_content['book_title'])

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=recomendation_data_content[['book_title', 'author']], k=5):


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

recomendation_data_content[recomendation_data_content.book_title.eq('The Diaries of Adam and Eve')]

recommendations = book_recommendations("The Diaries of Adam and Eve")

recommendations = recommendations.drop_duplicates()
recommendations

"""
### **Collaborative Filtered Recommendation System**
Dari data rating pengguna, maka akan diidentifikasi buku-buku yang mirip dan belum pernah dibaca oleh pengguna untuk direkomendasikan"""

preparation = preparation[preparation['Book_Rating'] != 0.0]
preparation

recomendation_data_collaborative = preparation.drop(columns=["Book_Author", "Book_Title", "Year_Of_Publication","Publisher"])
recomendation_data_collaborative.sort_values('User_ID')

"""### **Encoder data**"""

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = recomendation_data_collaborative['User_ID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah ISBN menjadi list tanpa nilai yang sama
isbn = recomendation_data_collaborative['ISBN'].unique().tolist()

# Melakukan proses encoding isbn
book_to_book_encoded = {x: i for i, x in enumerate(isbn)}

# Melakukan proses encoding angka ke isbn
book_encoded_to_book = {i: x for i, x in enumerate(isbn)}

# Selanjutnya, petakan userId dan isbn ke dataframe yang berkaitan.

# Mapping userId ke dataframe user
recomendation_data_collaborative['user'] = recomendation_data_collaborative['User_ID'].map(user_to_user_encoded)

# Mapping isbn ke dataframe books
recomendation_data_collaborative['isbn'] = recomendation_data_collaborative['ISBN'].map(book_to_book_encoded)

import numpy as np

num_users = len(user_to_user_encoded)
print(num_users)

num_book = len(book_encoded_to_book)
print(num_book)

recomendation_data_collaborative['ratings'] = recomendation_data_collaborative['Book_Rating'].values.astype(np.float32)

min_rating = min(recomendation_data_collaborative['Book_Rating'])

max_rating = max(recomendation_data_collaborative['Book_Rating'])

print('Number of User: {}, Number of book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

recomendation_data_collaborative = recomendation_data_collaborative.sample(frac=1, random_state=42)
recomendation_data_collaborative

"""### **Membagi data atau Split Data Train dan Test**"""

# Membuat variabel x untuk mencocokkan data user  dan book menjadi satu value
x = recomendation_data_collaborative[['user', 'isbn']].values.astype('float32')

# Membuat variabel y untuk membuat ratings dari hasil
y = recomendation_data_collaborative['ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values.astype('float32')

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * recomendation_data_collaborative.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)



# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 64,
    epochs = 100,
    validation_data = (x_val, y_val)
)

from pathlib import Path
import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

book_df = preparation
df = pd.read_csv('/content/Ratings.csv')
df = df.rename(columns={'User-ID': 'User_ID', "Book-Rating":"Book_Rating"})


user_id = df.User_ID.sample(1).iloc[0]
book_read_by_user = df[df.User_ID == user_id]


book_not_read = book_df[~book_df['ISBN'].isin(book_read_by_user.ISBN.values)]['ISBN']
book_not_read = list(
    set(book_not_read)
    .intersection(set(book_to_book_encoded.keys()))
)

book_not_read = [[book_to_book_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings = model.predict(user_book_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    book_encoded_to_book.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('books with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book_Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['ISBN'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.Book_Title, ':', row.Book_Author)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

recommended_book = book_df[book_df['ISBN'].isin(recommended_book_ids)]
for row in recommended_book.itertuples():
    print(row.Book_Title, ':', row.User_ID)